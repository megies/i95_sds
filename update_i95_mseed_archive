#!/usr/bin/env python
"""
Calculate I95 based on SDS waveform archive and FDSN server with station
metadata and store it in SDS structure in npy format.
"""
from __future__ import print_function
import json
import logging
import logging.handlers
import os
import pprint
import sys
import warnings
from argparse import ArgumentParser, RawDescriptionHelpFormatter

# we're not plotting anything, and obspy should also not make any matplotlib
# import that accidentally could set a backend, so this is really just in
# case.. to avoid unwanted interactive backends set on servers (which would
# lead to errors)
import matplotlib as mpl
mpl.use("AGG")
import matplotlib.pyplot as plt

import numpy as np

from obspy import UTCDateTime
from obspy.clients.filesystem.sds import Client as SDSClient
from obspy.clients.fdsn import Client as FDSNClient
from obspy.clients.fdsn.header import FDSNException


# STREAMS = ["EH", "HH", "EL"]
# COMPONENTS = "ZNE123"
#
# # length of analysis window in minutes
# WINDOW_LENGTH_MINUTES = 10
# WINDOW_LENGTH = WINDOW_LENGTH_MINUTES * 60
# # length of buffer before start / after end for preprocessing,
# # cut off before analysis, in seconds
# BUFFER_LENGTH = 1 * 60
# # instrument correction parameters
# WATER_LEVEL = 10
# PRE_FILT = (0.05, 0.1, 50, 60)
# FILTER_PARAMETERS = {
#     'type': "bandpass",
#     'freqmin': 0.5,
#     'freqmax': 30,
#     'corners': 4,
#     'zerophase': False,
#     }
#
# OVERWRITE = True
# SDS_ROOT_MSEED = "/bay200/mseed_online/archive"
# SDS_ROOT_I95 = "/bay200/I95_0.5-30Hz"
# FDSN_BASE_URL = "http://jane.geophysik.uni-muenchen.de"


DTYPE = np.dtype([('i95', np.float32), ('coverage', np.uint8)])
I95_PARAM_FILENAME = 'parameters.json'
I95_TIMES_FILENAME = 'times.npy'


EXAMPLE_PARAMETERS_JSON = """
{
    "buffer_length": 120,
    "fdsn_base_url": "http://jane",
    "filter_kwargs": {
        "corners": 4,
        "freqmax": 20.0,
        "freqmin": 1.0,
        "type": "bandpass",
        "zerophase": false
    },
    "pre_filt": [
        0.05,
        0.1,
        50.0,
        60.0
    ],
    "sds_root_waveforms": "/bay200/mseed_online/archive",
    "water_level": 10.0,
    "window_length_minutes": 10
}
"""


def _string_format_params(params):
    return "  " + "\n  ".join(pprint.pformat(params).splitlines())


def _get_metadata_from_i95_root(directory):
    i95_param_file = os.path.join(directory, I95_PARAM_FILENAME)
    i95_times_file = os.path.join(directory, I95_TIMES_FILENAME)
    # read processing parameters
    with open(i95_param_file, 'rb') as fh:
        json_string = fh.read().decode('ASCII')
    params = json.loads(json_string, encoding='ASCII')

    # read times-of-day
    if os.path.exists(i95_times_file):
        times_of_day = np.load(i95_times_file)
    # or create and also write them to file if not present
    else:
        # arbitrary date.. we only create times-of-day anyway
        t = UTCDateTime(2017, 1, 1)
        # time-of-day in nanoseconds to represent times corresponding to I95
        # values
        times = np.arange(t._ns, (t + 24 * 3600)._ns,
                          int(params['window_length_minutes'] * 60 * 1e9),
                          dtype=np.int64)
        times_of_day = times - t._ns
        # make info files in I95 SDS root read-only for everybody
        old_umask = os.umask(0222)
        try:
            # store times as plain ndarray in npy
            np.save(i95_times_file, times_of_day)
        finally:
            os.umask(old_umask)

    return params, times_of_day


def _write_result_npy(filename, data):
    outdir = os.path.dirname(filename)
    if not os.path.isdir(outdir):
        os.makedirs(outdir)
    if np.all(data['coverage'] == 0) and np.all(np.isnan(data['i95'])):
        # on empty data, write file with only content '0'
        # (would be easier to just touch an empty file but that could
        #  potentially be mixed up / mask other problems like file opening
        #  errors)
        with open(filename, 'wb') as fh:
            fh.write('0'.encode('ASCII'))
    else:
        np.save(filename, data)


def _plot_debug_image(t, i, tr, label, sds_root_i95):
    t = UTCDateTime(ns=t)
    directory = os.path.join(sds_root_i95, 'debug', tr.id, t.strftime("%Y-%j"))
    if not os.path.exists(directory):
        os.makedirs(directory)
    filename = os.path.join(directory, '{!s}_{!s}_{}.png'.format(
        t, i, label))
    tr.plot(outfile=filename)
    plt.close('all')


def main(argv=None):
    parser = ArgumentParser(
        prog='update_i95_mseed_archive', description=__doc__.strip(),
        formatter_class=RawDescriptionHelpFormatter)

    options = parser.add_argument_group(
        'General Options', 'General options controlling what data is '
        'processed and where results are stored.')
    options.add_argument(
        '-t', '--time', dest='time', required=True,
        help='Year and day to process (e.g. "2017-12-24" or "2017-365").')
    # options.add_argument(
    #     '--sds-root-waveforms', dest='sds_root_waveforms', required=True,
    #     type=str,
    #     help='Root directory of the SDS directory structure containing the '
    #          'waveform data (should contain subdirectories with years).')
    options.add_argument(
        '--sds-root-i95', dest='sds_root_i95', required=True, type=str,
        help='Root directory of the SDS directory structure used to store I95 '
             'data.')
    # options.add_argument(
    #     '--fdsn-base-url', dest='fdsn_base_url', required=True, type=str,
    #     help='FDSN base url used in obspy FDSN Client. This server will be '
    #          'used to fetch station metadata used in deconvolution.')
    options.add_argument(
        '--stream', dest='streams', default=[], action='append', required=True,
        help='Stream to consider (i.e. first two characters of SEED channel '
             'code). This option can be specified multiple times (e.g. use '
             '"-s HH -s EH -s EL"). Other streams that are not specified will '
             'be ignored (e.g. in the example, lower sample rate streams like '
             'BHZ')
    options.add_argument(
        '--component-codes', dest='component_codes', type=str,
        default='ZNE123',
        help='Specifies which component codes are considered (i.e. last '
             'character of SEED channel code).')
    options.add_argument(
        '--station-code', dest='station_codes', default=[], action='append',
        help='Specifies whether only specific station(s) (SEED station code) '
             'should be considered. If not used, all stations available at a '
             'given day will be used. If specified, only the given station(s) '
             'will be processed. Can be specified multiple times to consider '
             'a few select stations.')
    options.add_argument(
        '--overwrite', dest='overwrite', default=False,
        action='store_true',
        help='Whether to overwrite already existing data. This can be used to '
             'force an update of already existing data (e.g. when archive '
             'data is added at a later point). Otherwise a given day is not '
             'processed if output for that day is already present.')
    options.add_argument(
        '-v', '--verbose', dest='verbose', default=False,
        action="store_true",
        help='Print verbose output while processing files.')
    options.add_argument(
        '--debug', dest='debug', default=False,
        action="store_true",
        help='Plot images to files in /tmp during processing.')
    options.add_argument(
        '--logfile-suffix', dest='logfile_suffix', default='', type=str,
        help='Suffix for filename of log file (to log multiple simultaneous '
             'program runs to separate files).')

    # All of this is now handled via the parameter.json file
    # options = parser.add_argument_group(
    #     '(Pre-)Processing Options', 'Settings regarding the details of '
    #     '(pre-)processing. An exception will be raised when trying to add '
    #     'data to an existing I95 SDS archive and a mismatch of processing '
    #     'settings is detected.')
    # options.add_argument(
    #     '--window-length', dest='window_length_minutes', type=float,
    #     default=10,
    #     help='Specifies length of data window the I95 value is based on in '
    #          'minutes (this is the actual length, buffers will be added on '
    #          'top of this window and cut off after pre-processing).')
    # options.add_argument(
    #     '--buffer-length', dest='buffer_length', type=float,
    #     default=120,
    #     help='Specifies length of waveform buffer both before and after '
    #          'actual window used during preprocessing in seconds.')
    # options.add_argument(
    #     '--water-level', dest='water_level', required=True,
    #     help='Specifies "water_level" option in obspy '
    #          '"Stream.remove_response()" during preprocessing. Use a float '
    #          'value or "None" for no water level used in deconvolution.')
    # options.add_argument(
    #     '--pre-filt', dest='pre_filt', required=True,
    #     help='Specifies "pre_filt" option in obspy "Stream.remove_response()" '
    #          'during preprocessing. Use four comma-separated floats (no '
    #          'spaces, e.g. "0.05,0.1,50,60") or "None" for no pre filter used '
    #          'in deconvolution.')
    # options.add_argument(
    #     '--filter-type', dest='filter_type', required=True, type=str,
    #     help='Specifies which filter to use after deconvolution and before '
    #          'calculating I95. Use either "None", "bandpass", "lowpass" or '
    #          '"highpass". See obspy Stream.filter().')
    # options.add_argument(
    #     '--filter-freq', dest='filter_freq', default=None, type=float,
    #     help='Specifies "freq" option for used filter. Only used when '
    #          '"lowpass" or "highpass" is selected.')
    # options.add_argument(
    #     '--filter-freqmin', dest='filter_freqmin', default=None, type=float,
    #     help='Specifies "freqmin" option for used filter. Only used when '
    #          '"bandpass" is selected.')
    # options.add_argument(
    #     '--filter-freqmax', dest='filter_freqmax', default=None, type=float,
    #     help='Specifies "freqmax" option for used filter. Only used when '
    #          '"bandpass" is selected.')
    # options.add_argument(
    #     '--filter-corners', dest='filter_corners', default=None, type=int,
    #     help='Specifies "corners" option for used filter.')
    # options.add_argument(
    #     '--filter-zerophase', dest='filter_zerophase', default=False,
    #     action='store_true',
    #     help='Whether to use a two-pass zero-phase filter (default is no '
    #          'zero-phase). Filter is applied twice if this option is '
    #          'selected.')

    args = parser.parse_args(argv)
    verbose = args.verbose
    debug = args.debug

    t = UTCDateTime(args.time)
    # work on the full day of the given timestamp
    year_day_string = t.strftime("%Y-%j")
    t = UTCDateTime(year_day_string)
    t_end = t + 24 * 3600

    if t >= UTCDateTime():
        msg = 'Time to process is in the future ({!s})'.format(t)
        raise Exception(msg)

    # check if everything is in place regarding root directory
    if not os.path.isdir(args.sds_root_i95):
        msg = ('I95 SDS root directory "{}" does not exist or is no '
               'directory.')
        raise Exception(msg.format(args.sds_root_i95))
    # check if parameters.json is in place
    i95_param_file = os.path.join(args.sds_root_i95, I95_PARAM_FILENAME)
    if not os.path.exists(i95_param_file):
        msg = ('I95 SDS root directory "{}" does not contain mandatory '
               '"parameters.json" file. Create file with e.g. following '
               'content:\n\n{}\n')
        raise Exception(msg.format(args.sds_root_i95, EXAMPLE_PARAMETERS_JSON))
    params, times_of_day = _get_metadata_from_i95_root(args.sds_root_i95)
    times = times_of_day + t._ns

    if params['filter_kwargs']['type']:
        use_filter = True
    else:
        use_filter = False

    # set up logger
    if args.logfile_suffix:
        log_filename = 'log_{}.txt'.format(args.logfile_suffix)
    else:
        log_filename = 'log.txt'
    global logger
    logger = logging.getLogger()
    if debug:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
    handler = logging.handlers.RotatingFileHandler(
        os.path.join(args.sds_root_i95, log_filename), maxBytes=10e6)
    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    # logger.info(msg)
    # if verbose:
    #     print(msg)

    msg = 'Processing ' + year_day_string
    logger.info(msg)
    if verbose:
        print(msg)

    sds_client_mseed = SDSClient(params['sds_root_waveforms'])
    fdsn_client = FDSNClient(params['fdsn_base_url'])
    sds_client_npy = SDSClient(args.sds_root_i95)
    sds_client_npy.FMTSTR += ".npy"
    msg = 'Connected SDS/FDSN clients.'
    logger.info(msg)
    if verbose:
        print(msg)

    inv_all = fdsn_client.get_stations(
        level='channel', format='text', starttime=t - 100, endtime=t_end + 100)
    msg = 'Fetched station inventory summary from FDSN client.'
    logger.info(msg)
    if verbose:
        print(msg)

    # replaced because it is so slow..
    #  .. lookup SEED codes from fetched bare inventory
    # nslc = [(n, s, l, c)
    #         for (n, s, l, c) in sds_client_mseed.get_all_nslc(datetime=t)
    #         if c[:2] in args.streams and c[2] in args.component_codes]
    nslc = set()
    for net in inv_all:
        for sta in net:
            if args.station_codes and sta.code not in args.station_codes:
                continue
            for cha in sta:
                if cha.code[:2] not in args.streams:
                    continue
                if cha.code[2] not in args.component_codes:
                    continue
                nslc.add((net.code, sta.code, cha.location_code, cha.code))
    msg = 'Processing the following SEED IDs: ' + ' '.join(
        ('.'.join(nslc_) for nslc_ in nslc))
    logger.info(msg)
    if verbose:
        print(msg)

    def _process(net, sta, loc, cha, t, times, params, verbose=verbose,
                 debug=debug):
        global logger
        npy_file = sds_client_npy._get_filename(
            net, sta, loc, cha, t)

        if not args.overwrite and os.path.exists(npy_file):
            msg = ("Outfile exists and overwrite option not specified, "
                   "skipping: {}".format(npy_file))
            logger.info(msg)
            if verbose:
                print(msg)
            return

        inv = None
        try:
            inv = fdsn_client.get_stations(
                network=net, station=sta, location=loc, channel=cha,
                level="response", starttime=t - 100, endtime=t_end + 100)
        except FDSNException as e:
            if verbose:
                print('failed!')
            msg = ("Failed to fetch station metadata for '{}': "
                   "{}").format('.'.join(net, sta, loc, cha), str(e))
            logger.error(msg)
            warnings.warn(msg)
            return
        msg = ('Fetched response information from FDSN client.')
        logger.info(msg)
        if verbose:
            print(msg)

        data = np.zeros(len(times), dtype=DTYPE)

        window_length = params['window_length_minutes'] * 60

        msg = '{} windows to process'.format(len(times))
        logger.info(msg)
        if verbose:
            print(msg + ': ', end='')
        for i, t_ in enumerate(times):
            if verbose:
                if verbose and i % 5 == 0:
                    print(str(i), end='')
            data_ = []
            #    t0 ----------- t1 --------------------- t2 ----------- t3
            #          buffer           actual window          buffer
            t1 = UTCDateTime(ns=t_) - (window_length / 2.0)
            t2 = UTCDateTime(ns=t_) + (window_length / 2.0)
            t0 = t1 - params['buffer_length']
            t3 = t2 + params['buffer_length']
            st = sds_client_mseed.get_waveforms(net, sta, loc, cha, t0, t3)
            # cleanup merge..
            st.merge(-1)
            # now throw away gaps..
            st.merge(0, fill_value=None)
            st = st.split()
            # omit traces that are less than twice the buffer length and thus
            # can not be processed..
            st.traces = [
                tr for tr in st if
                (tr.stats.endtime - tr.stats.starttime) >
                2 * params['buffer_length']]
            if not st:
                if verbose:
                    print('x', end='')
                data['i95'][i] = np.nan
                data['coverage'][i] = 0
                continue
            coverage_ = 0
            for j, tr in enumerate(st):
                if debug:
                    _plot_debug_image(t_, j, tr, '0_raw', args.sds_root_i95)
                tr.detrend("demean")
                tr.taper(type="cosine", max_percentage=None,
                         max_length=params['buffer_length'] * 0.8, side="both")
                tr.remove_response(
                    inventory=inv, output="VEL",
                    water_level=params['water_level'],
                    pre_filt=params['pre_filt'], zero_mean=False, taper=False)
                if debug:
                    _plot_debug_image(t_, j, tr, '1_deconvolved',
                                      args.sds_root_i95)
                if use_filter:
                    tr.filter(**params['filter_kwargs'])
                if debug:
                    _plot_debug_image(t_, j, tr, '2_filtered',
                                      args.sds_root_i95)
                tr.detrend("demean")
                # make sure that tapered parts are removed. for traces that do
                # not fully span the expected window (gappy data), we need to
                # cut away part of the actual data window to avoid having
                # tapered data in the I95 calculation (which would lead to
                # artificially lowered I95 for gappy data)
                trim_start = max(
                    t1, tr.stats.starttime + params['buffer_length'])
                trim_end = min(t2, tr.stats.endtime - params['buffer_length'])
                tr.trim(starttime=trim_start, endtime=trim_end)
                if debug:
                    _plot_debug_image(t_, j, tr, '3_trimmed',
                                      args.sds_root_i95)
                coverage_ += (
                    tr.stats.endtime - tr.stats.starttime) / window_length
                data_.append(np.abs(tr.data) * 1e9)  # work in nm/s
            data_ = np.concatenate(data_)
            data['i95'][i] = np.percentile(data_, 95)
            # save coverage in percent
            data['coverage'][i] = round(coverage_ * 100)
            if verbose:
                print('.', end='')
            sys.stdout.flush()
        if verbose:
            print(' done!')

        _write_result_npy(npy_file, data)

    total = len(nslc)
    for i, (net, sta, loc, cha) in enumerate(sorted(nslc)):
        msg = 'Processing SEED ID {}  ({!s}/{!s})'.format(
            '.'.join((net, sta, loc, cha)), i+1, total)
        logger.info(msg)
        if verbose:
            print(msg)
        _process(net, sta, loc, cha, t, times, params, verbose=verbose)


if __name__ == '__main__':
    main()
