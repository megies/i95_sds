#!/usr/bin/env python3
"""
Calculate I95 based on SDS waveform archive and FDSN server with station
metadata and store it in SDS structure in npy format.
"""
import json
import logging
import logging.handlers
import os
import pprint
import sys
import warnings
from argparse import ArgumentParser, RawDescriptionHelpFormatter

# we're not plotting anything, and obspy should also not make any matplotlib
# import that accidentally could set a backend, so this is really just in
# case.. to avoid unwanted interactive backends set on servers (which would
# lead to errors)
# XXX The following should not be needed anymore, since obspy for a long time
# XXX has taken care not to import any matplotlib things that might
# XXX inadvertently set an interactive backend, but if problems really occur
# XXX this can be commented out as a fallback
# XXX import matplotlib as mpl
# XXX mpl.use("AGG")
# XXX import matplotlib.pyplot as plt

import numpy as np

from obspy import UTCDateTime, Inventory, read_inventory
from obspy.clients.filesystem.sds import Client as SDSClient
from obspy.clients.fdsn import Client as FDSNClient
from obspy.clients.fdsn.header import FDSNException


DTYPE = np.dtype([('i95', np.float32), ('coverage', np.uint8)])
I95_PARAM_FILENAME = 'parameters.json'
I95_TIMES_FILENAME = 'times.npy'


EXAMPLE_PARAMETERS_JSON = """
{
    "buffer_length": 120,
    "fdsn_base_url": "https://erde.geophysik.uni-muenchen.de",
    "inventories": [
        "/path/to/some/station_metadata.xml",
        "/path/to/some/station_metadata2.xml"
    ],
    "filter_kwargs": {
        "corners": 4,
        "freqmax": 20.0,
        "freqmin": 1.0,
        "type": "bandpass",
        "zerophase": false
    },
    "pre_filt": [
        0.05,
        0.1,
        50.0,
        60.0
    ],
    "sds_root_waveforms": "/bay200/mseed_online/archive",
    "water_level": None,
    "window_length_minutes": 10
}
"""


def _string_format_params(params):
    return "  " + "\n  ".join(pprint.pformat(params).splitlines())


def _get_metadata_from_i95_root(directory):
    i95_param_file = os.path.join(directory, I95_PARAM_FILENAME)
    i95_times_file = os.path.join(directory, I95_TIMES_FILENAME)
    # read processing parameters
    with open(i95_param_file, 'rb') as fh:
        json_string = fh.read().decode('ASCII')
    params = json.loads(json_string, encoding='ASCII')

    # read times-of-day
    if os.path.exists(i95_times_file):
        times_of_day = np.load(i95_times_file)
    # or create and also write them to file if not present
    else:
        # arbitrary date.. we only create times-of-day anyway
        t = UTCDateTime(2017, 1, 1)
        # time-of-day in nanoseconds to represent times corresponding to I95
        # values
        times = np.arange(t._ns, (t + 24 * 3600)._ns,
                          int(params['window_length_minutes'] * 60 * 1e9),
                          dtype=np.int64)
        times_of_day = times - t._ns
        # make info files in I95 SDS root read-only for everybody
        old_umask = os.umask(0o222)
        try:
            # store times as plain ndarray in npy
            np.save(i95_times_file, times_of_day)
        finally:
            os.umask(old_umask)

    return params, times_of_day


def _write_result_npy(filename, data):
    outdir = os.path.dirname(filename)
    if not os.path.isdir(outdir):
        os.makedirs(outdir)
    if np.all(data['coverage'] == 0) and np.all(np.isnan(data['i95'])):
        # on empty data, write file with only content '0'
        # (would be easier to just touch an empty file but that could
        #  potentially be mixed up / mask other problems like file opening
        #  errors)
        with open(filename, 'wb') as fh:
            fh.write('0'.encode('ASCII'))
    else:
        np.save(filename, data)


def _plot_debug_image(t, i, tr, label, sds_root_i95):
    import matplotlib.pyplot as plt
    t = UTCDateTime(ns=t)
    directory = os.path.join(sds_root_i95, 'debug', tr.id, t.strftime("%Y-%j"))
    if not os.path.exists(directory):
        os.makedirs(directory)
    filename = os.path.join(directory, '{!s}_{!s}_{}.png'.format(
        t, i, label))
    tr.plot(outfile=filename)
    plt.close('all')


def main(argv=None):
    parser = ArgumentParser(
        prog='update_i95_mseed_archive', description=__doc__.strip(),
        formatter_class=RawDescriptionHelpFormatter)

    options = parser.add_argument_group(
        'General Options', 'General options controlling what data is '
        'processed and where results are stored.')
    options.add_argument(
        '-t', '--time', dest='time', required=True,
        help='Year and day to process (e.g. "2017-12-24" or "2017-365").')
    options.add_argument(
        '--sds-root-i95', dest='sds_root_i95', required=True, type=str,
        help='Root directory of the SDS directory structure used to store I95 '
             'data.')
    options.add_argument(
        '--stream', dest='streams', default=[], action='append', required=True,
        help='Stream to consider (i.e. first two characters of SEED channel '
             'code). This option can be specified multiple times (e.g. use '
             '"--streams HH --streams EH --streams EL"). '
             'Other streams that are not specified will '
             'be ignored (e.g. in the example, lower sample rate streams like '
             'BHZ')
    options.add_argument(
        '--component-codes', dest='component_codes', type=str,
        default='ZNE123',
        help='Specifies which component codes are considered (i.e. last '
             'character of SEED channel code).')
    options.add_argument(
        '--station-code', dest='station_codes', default=[], action='append',
        help='Specifies whether only specific station(s) (SEED station code) '
             'should be considered. If not used, all stations available at a '
             'given day will be used. If specified, only the given station(s) '
             'will be processed. Can be specified multiple times to consider '
             'a few select stations.')
    options.add_argument(
        '--overwrite', dest='overwrite', default=False,
        action='store_true',
        help='Whether to overwrite already existing data. This can be used to '
             'force an update of already existing data (e.g. when archive '
             'data is added at a later point). Otherwise a given day is not '
             'processed if output for that day is already present.')
    options.add_argument(
        '-v', '--verbose', dest='verbose', default=False,
        action="store_true",
        help='Print verbose output while processing files.')
    options.add_argument(
        '--debug', dest='debug', default=False,
        action="store_true",
        help='Plot images to files in /tmp during processing.')
    options.add_argument(
        '--logfile-suffix', dest='logfile_suffix', default='', type=str,
        help='Suffix for filename of log file (to log multiple simultaneous '
             'program runs to separate files).')

    args = parser.parse_args(argv)
    verbose = args.verbose
    debug = args.debug

    t = UTCDateTime(args.time)
    # work on the full day of the given timestamp
    year_day_string = t.strftime("%Y-%j")
    t = UTCDateTime(year_day_string)
    t_end = t + 24 * 3600

    if t >= UTCDateTime():
        msg = 'Time to process is in the future ({!s})'.format(t)
        raise Exception(msg)

    # check if everything is in place regarding root directory
    if not os.path.isdir(args.sds_root_i95):
        msg = ('I95 SDS root directory "{}" does not exist or is no '
               'directory.')
        raise Exception(msg.format(args.sds_root_i95))
    # check if parameters.json is in place
    i95_param_file = os.path.join(args.sds_root_i95, I95_PARAM_FILENAME)
    if not os.path.exists(i95_param_file):
        msg = ('I95 SDS root directory "{}" does not contain mandatory '
               '"parameters.json" file. Create file with e.g. following '
               'content:\n\n{}\n')
        raise Exception(msg.format(args.sds_root_i95, EXAMPLE_PARAMETERS_JSON))
    params, times_of_day = _get_metadata_from_i95_root(args.sds_root_i95)
    times = times_of_day + t._ns

    if params['filter_kwargs']['type']:
        use_filter = True
    else:
        use_filter = False

    # set up logger
    if args.logfile_suffix:
        log_filename = 'log_{}.txt'.format(args.logfile_suffix)
    else:
        log_filename = 'log.txt'
    global logger
    logger = logging.getLogger()
    if debug:
        import matplotlib as mpl
        mpl.use("AGG")
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
    handler = logging.handlers.RotatingFileHandler(
        os.path.join(args.sds_root_i95, log_filename), maxBytes=10e6)
    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    # logger.info(msg)
    # if verbose:
    #     print(msg)

    msg = 'Processing ' + year_day_string
    logger.info(msg)
    if verbose:
        print(msg)

    sds_client_mseed = SDSClient(params['sds_root_waveforms'])
    if params['fdsn_base_url'] is None:
        fdsn_client = None
    else:
        fdsn_client = FDSNClient(params['fdsn_base_url'])
    sds_client_npy = SDSClient(args.sds_root_i95)
    sds_client_npy.FMTSTR += ".npy"
    msg = 'Connected SDS/FDSN clients.'
    logger.info(msg)
    if verbose:
        print(msg)

    if fdsn_client:
        inv_all = fdsn_client.get_stations(
            level='channel', format='text', starttime=t - 100,
            endtime=t_end + 100)
    else:
        inv_all = Inventory(networks=[], source='')
    if 'inventories' in params and params['inventories']:
        inv_local_files = Inventory(networks=[], source='')
        for path in params['inventories']:
            inv_local_files += read_inventory(path, format='STATIONXML')
        inv_all += inv_local_files
    else:
        inv_local_files = None
    msg = 'Fetched station inventory summary from FDSN client.'
    logger.info(msg)
    if verbose:
        print(msg)

    # replaced because it is so slow..
    #  .. lookup SEED codes from fetched bare inventory
    # nslc = [(n, s, l, c)
    #         for (n, s, l, c) in sds_client_mseed.get_all_nslc(datetime=t)
    #         if c[:2] in args.streams and c[2] in args.component_codes]
    nslc = set()
    for net in inv_all:
        for sta in net:
            if args.station_codes and sta.code not in args.station_codes:
                continue
            for cha in sta:
                if cha.code[:2] not in args.streams:
                    continue
                if cha.code[2] not in args.component_codes:
                    continue
                nslc.add((net.code, sta.code, cha.location_code, cha.code))
    msg = 'Processing the following SEED IDs: ' + ' '.join(
        ('.'.join(nslc_) for nslc_ in nslc))
    logger.info(msg)
    if verbose:
        print(msg)

    def _process(net, sta, loc, cha, t, times, params, verbose=verbose,
                 debug=debug):
        global logger
        npy_file = sds_client_npy._get_filename(
            net, sta, loc, cha, t)

        if not args.overwrite and os.path.exists(npy_file):
            msg = ("Outfile exists and overwrite option not specified, "
                   "skipping: {}".format(npy_file))
            logger.info(msg)
            if verbose:
                print(msg)
            return

        inv = None
        if inv_local_files:
            inv = inv_local_files.select(
                network=net, station=sta, location=loc, channel=cha,
                starttime=t - 100, endtime=t_end + 100)
            if inv:
                msg = ('Found response information in data from local '
                       'inventory files.')
                logger.info(msg)
        if not inv and fdsn_client:
            try:
                inv = fdsn_client.get_stations(
                    network=net, station=sta, location=loc, channel=cha,
                    level="response", starttime=t - 100, endtime=t_end + 100)
            except FDSNException as e:
                if verbose:
                    print('failed!')
                msg = ("Failed to fetch station metadata for '{}': "
                       "{}").format('.'.join((net, sta, loc, cha)), str(e))
                logger.error(msg)
                warnings.warn(msg)
                return
            msg = ('Fetched response information from FDSN client.')
            logger.info(msg)
        if verbose:
            print(msg)

        data = np.zeros(len(times), dtype=DTYPE)

        window_length = params['window_length_minutes'] * 60

        msg = '{} windows to process'.format(len(times))
        logger.info(msg)
        if verbose:
            print(msg + ': ', end='')
        for i, t_ in enumerate(times):
            if verbose:
                if verbose and i % 5 == 0:
                    print(str(i), end='')
            data_ = []
            #    t0 ----------- t1 --------------------- t2 ----------- t3
            #          buffer           actual window          buffer
            t1 = UTCDateTime(ns=t_) - (window_length / 2.0)
            t2 = UTCDateTime(ns=t_) + (window_length / 2.0)
            t0 = t1 - params['buffer_length']
            t3 = t2 + params['buffer_length']
            st = sds_client_mseed.get_waveforms(net, sta, loc, cha, t0, t3)
            # cleanup merge..
            st.merge(-1)
            # now throw away gaps..
            st.merge(0, fill_value=None)
            st = st.split()
            # omit traces that are less than twice the buffer length and thus
            # can not be processed..
            st.traces = [
                tr for tr in st if
                (tr.stats.endtime - tr.stats.starttime) >
                2 * params['buffer_length']]
            if not st:
                if verbose:
                    print('x', end='')
                data['i95'][i] = np.nan
                data['coverage'][i] = 0
                continue
            coverage_ = 0
            for j, tr in enumerate(st):
                if debug:
                    _plot_debug_image(t_, j, tr, '0_raw', args.sds_root_i95)
                tr.detrend("demean")
                tr.taper(type="cosine", max_percentage=None,
                         max_length=params['buffer_length'] * 0.8, side="both")
                try:
                    tr.remove_response(
                        inventory=inv, output="VEL",
                        water_level=params['water_level'],
                        pre_filt=params['pre_filt'], zero_mean=False, taper=False)
                except:
                    import pdb; pdb.set_trace()
                if debug:
                    _plot_debug_image(t_, j, tr, '1_deconvolved',
                                      args.sds_root_i95)
                if use_filter:
                    tr.filter(**params['filter_kwargs'])
                if debug:
                    _plot_debug_image(t_, j, tr, '2_filtered',
                                      args.sds_root_i95)
                tr.detrend("demean")
                # make sure that tapered parts are removed. for traces that do
                # not fully span the expected window (gappy data), we need to
                # cut away part of the actual data window to avoid having
                # tapered data in the I95 calculation (which would lead to
                # artificially lowered I95 for gappy data)
                trim_start = max(
                    t1, tr.stats.starttime + params['buffer_length'])
                trim_end = min(t2, tr.stats.endtime - params['buffer_length'])
                tr.trim(starttime=trim_start, endtime=trim_end)
                if debug:
                    _plot_debug_image(t_, j, tr, '3_trimmed',
                                      args.sds_root_i95)
                coverage_ += (
                    tr.stats.endtime - tr.stats.starttime) / window_length
                data_.append(np.abs(tr.data) * 1e9)  # work in nm/s
            data_ = np.concatenate(data_)
            data['i95'][i] = np.percentile(data_, 95)
            # save coverage in percent
            data['coverage'][i] = round(coverage_ * 100)
            if verbose:
                print('.', end='')
            sys.stdout.flush()
        if verbose:
            print(' done!')

        _write_result_npy(npy_file, data)

    total = len(nslc)
    for i, (net, sta, loc, cha) in enumerate(sorted(nslc)):
        msg = 'Processing SEED ID {}  ({!s}/{!s})'.format(
            '.'.join((net, sta, loc, cha)), i+1, total)
        logger.info(msg)
        if verbose:
            print(msg)
        _process(net, sta, loc, cha, t, times, params, verbose=verbose)


if __name__ == '__main__':
    main()
